{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSL progress.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNvXdHwFHv7Kjh70JfF+KBV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shimels1/node-assessment/blob/master/CSL_progress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTQkhewQ-jW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc7494e-b87f-4178-ab9c-ca8fcab882d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MTSHhKI_ovI"
      },
      "source": [
        "import cv2\n",
        "import time\n",
        "import numpy\n",
        "import csv\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import numpy\n",
        "from PIL import Image\n",
        "# filter warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "import keras\n",
        "import sys\n",
        "# keras imports\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.layers import Input\n",
        "import csv\n",
        "from keras.preprocessing import image\n",
        "# other imports\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "#import glob\n",
        "# import cv2\n",
        "import h5py\n",
        "import os\n",
        "#import json\n",
        "import datetime\n",
        "import time\n",
        "import effTools as eff\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D10Gkpde_tzt"
      },
      "source": [
        "face_cascades = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hipqh6-BBA90"
      },
      "source": [
        "def median_filter(data, filter_size):\n",
        "    temp = []\n",
        "    indexer = filter_size // 2\n",
        "    data_final = []\n",
        "    data_final = numpy.zeros((len(data),len(data[0])))\n",
        "    for i in range(len(data)):\n",
        "\n",
        "        for j in range(len(data[0])):\n",
        "\n",
        "            for z in range(filter_size):\n",
        "                if i + z - indexer < 0 or i + z - indexer > len(data) - 1:\n",
        "                    for c in range(filter_size):\n",
        "                        temp.append(0)\n",
        "                else:\n",
        "                    if j + z - indexer < 0 or j + indexer > len(data[0]) - 1:\n",
        "                        temp.append(0)\n",
        "                    else:\n",
        "                        for k in range(filter_size):\n",
        "                            temp.append(data[i + z - indexer][j + k - indexer])\n",
        "\n",
        "            temp.sort()\n",
        "            data_final[i][j] = temp[len(temp) // 2]\n",
        "            temp = []\n",
        "    return data_final"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEE4xtoY_xbA"
      },
      "source": [
        "# store all file name in the folder into one array list\n",
        "dataFileName = []\n",
        "\n",
        "for filename in os.listdir(\"/content/drive/MyDrive/Adjectives/99. healthy\"):\n",
        "  if filename.endswith(\"MOV\"): \n",
        "      dataFileName.append(filename)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlhgzUBJfqsR"
      },
      "source": [
        "# Part 2 - Building the CNN\n",
        "\n",
        "# Initialising the CNN\n",
        "cnn = Sequential()\n",
        "\n",
        "# Adding a 1nd convolutional layer\n",
        "cnn.add(Convolution2D(filters=32, kernel_size=3, activation='relu', input_shape=[224, 224, 3]))\n",
        "cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "# Adding a 2nd convolutional layer\n",
        "cnn.add(Convolution2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "# # Adding a 3rd convolutional layer\n",
        "# cnn.add(Convolution2D(filters=32, kernel_size=3, activation='relu'))\n",
        "# cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "# # Adding a 4rd convolutional layer\n",
        "# cnn.add(Convolution2D(filters=32, kernel_size=3, activation='relu'))\n",
        "# cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "cnn.add(Flatten())\n",
        "\n",
        "# cnn.add(Dense(64, activation='relu'))\n",
        "# # cnn.add(Dropout(0.3))\n",
        "\n",
        "# cnn.add(Dense(64, activation='relu'))\n",
        "# # cnn.add(Dropout(0.3))       \n",
        "# cnn.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# cnn.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "# cnn.summary()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQUNlisWgldA"
      },
      "source": [
        "# image preprocessing (resize image, nose removale using midian filter, feauter exstraction using cnn)\n",
        "for i in range(0,len(dataFileName)):\n",
        "    cap = cv2.VideoCapture(\"drive/MyDrive/Adjectives/99. healthy/\"+dataFileName[i])\n",
        "    if cap.isOpened() == False:\n",
        "        print(\"Error File Not Found\")\n",
        "    \n",
        "    c = 0\n",
        "  \n",
        "    while cap.isOpened():\n",
        "      success, img = cap.read()\n",
        "      # print(i)\n",
        "      if success == True:\n",
        "        \n",
        "        #save the result\n",
        "        orginalImage = cv2.resize(img, (224, 224))\n",
        "        cv2.imwrite(\"a.jpg\",orginalImage)\n",
        "\n",
        "        # Face detection and draw white background\n",
        "        gray_frame = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascades.detectMultiScale(gray_frame, 1.3, 5)\n",
        "\n",
        "        for (x, y, w, h) in faces:\n",
        "\n",
        "            # draw red box\n",
        "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), -1)\n",
        "            roi_gray = gray_frame[y:y + h, x:x + w]\n",
        "            roi_color = img[y:y + h, x:x + w]\n",
        "\n",
        "            #save the result  \n",
        "            FaceDetectionWithRedBackground = cv2.resize(img, (224, 224))\n",
        "            cv2.imwrite(\"b.jpg\",FaceDetectionWithRedBackground)\n",
        "\n",
        "            # converting from gbr to hsv color space\n",
        "            img_HSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "            # skin color range for hsv color space\n",
        "            HSV_mask = cv2.inRange(img_HSV, (0, 15, 0), (17, 170, 255))\n",
        "            HSV_mask = cv2.morphologyEx(HSV_mask, cv2.MORPH_OPEN, numpy.ones((3, 3), numpy.uint8))\n",
        "\n",
        "            # converting from gbr to YCbCr color space\n",
        "            img_YCrCb = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "            # skin color range for hsv color space\n",
        "            YCrCb_mask = cv2.inRange(img_YCrCb, (0, 135, 85), (255, 180, 135))\n",
        "            YCrCb_mask = cv2.morphologyEx(YCrCb_mask, cv2.MORPH_OPEN, numpy.ones((3, 3), numpy.uint8))\n",
        "\n",
        "            # merge skin detection (YCbCr and hsv)\n",
        "            global_mask = cv2.bitwise_and(YCrCb_mask, HSV_mask)\n",
        "            global_mask = cv2.medianBlur(global_mask, 3)\n",
        "            global_mask = cv2.morphologyEx(global_mask, cv2.MORPH_OPEN, numpy.ones((4, 4), numpy.uint8))\n",
        "\n",
        "            HSV_result = cv2.bitwise_not(HSV_mask)\n",
        "            YCrCb_result = cv2.bitwise_not(YCrCb_mask)\n",
        "            global_result = cv2.bitwise_not(global_mask)\n",
        "\n",
        "            # # MedianFilter\n",
        "            # arr = numpy.array(global_result)\n",
        "            # removed_noise = median_filter(arr, 3)\n",
        "            # global_result = np.asarray(removed_noise)\n",
        "\n",
        "            if c < 50:\n",
        "              global_result = cv2.resize(global_result, (224, 224))\n",
        "              cv2.imwrite(\"3_global_resultm.jpg\",global_result)\n",
        "              clear_output(wait=True)\n",
        "              # display the result\n",
        "              orginalImage=image.load_img('a.jpg', target_size = (224, 224))\n",
        "              FaceDetectionWithRedBackground=image.load_img('b.jpg', target_size = (224, 224))\n",
        "              FaceDetectionWithRedBackground = image.img_to_array(FaceDetectionWithRedBackground)\n",
        "              orginalImage = image.img_to_array(orginalImage)\n",
        "              cv2_imshow(orginalImage)\n",
        "              cv2_imshow(FaceDetectionWithRedBackground)\n",
        "              cv2_imshow(global_result)\n",
        "\n",
        "              test_image=image.load_img('3_global_resultm.jpg', target_size = (224, 224))\n",
        "              test_image = image.img_to_array(test_image)\n",
        "              test_image = np.expand_dims(test_image, axis = 0)\n",
        "              r1=cnn.predict(test_image)\n",
        "              a=np.array(r1[0])\n",
        "              a = numpy.append(a, \"healthy\")\n",
        "              with open('/content/drive/MyDrive/testdataset6.csv', 'a+', newline='') as f:\n",
        "                write = csv.writer(f)\n",
        "                write.writerow(a)\n",
        "              cv2.waitKey(1) & 0xff\n",
        "            \n",
        "            c = c + 1\n",
        "      else:\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWLPZze0vub7",
        "outputId": "779a71c3-ad25-4fa6-9503-042b15ff8f74"
      },
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# # Importing the training set\n",
        "dataset_train = pd.read_csv('/content/drive/MyDrive/testdataset6.csv')\n",
        "print(dataset_train.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31, 93313)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S28sopuLCq8K",
        "outputId": "efaaa9e3-281f-429a-9d4c-96f80dabc0ea"
      },
      "source": [
        "training_set = dataset_train.iloc[:, 0:93312].values\n",
        "training_set_y = dataset_train.iloc[:, 93312:93313].values\n",
        "\n",
        "training_test = dataset_train.iloc[:, 0:93312].values\n",
        "training_test_y = dataset_train.iloc[:, 93312:93313].values\n",
        "print(training_test)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[18.674448  0.       38.18519  ...  4.015486 57.176132  0.      ]\n",
            " [18.674448  0.       38.18519  ...  4.015486 57.176132  0.      ]\n",
            " [18.674448  0.       38.18519  ...  4.015486 57.176132  0.      ]\n",
            " ...\n",
            " [18.674448  0.       38.18519  ...  4.015486 57.176132  0.      ]\n",
            " [18.674448  0.       38.18519  ...  4.015486 57.176132  0.      ]\n",
            " [18.674448  0.       38.18519  ...  4.015486 57.176132  0.      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ6l9ZolKQ1O"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_set_scaled = sc.fit_transform(training_set)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of4EoDOcKTzn"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "training_test_scaled = sc.fit_transform(training_test)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkWhgcliJbFS"
      },
      "source": [
        "c=0\n",
        "k=0\n",
        "wordArry = []\n",
        "X = []\n",
        "y = []\n",
        "t_X = []\n",
        "t_y = []\n",
        "dry = 0\n",
        "sick = 0\n",
        "healthy = 0\n",
        "for i in range(0,len(training_set_scaled)):\n",
        "  c = c + 1\n",
        "  wordArry.append(training_set_scaled[i])\n",
        "  if ( c == 50 ):\n",
        "    if training_set_y[i] == 'dry':\n",
        "      if dry < 14:\n",
        "        X.append(wordArry)\n",
        "        y.append(0)\n",
        "        dry = dry + 1\n",
        "      else:\n",
        "        t_X.append(wordArry)\n",
        "        t_y.append(0)\n",
        "    if training_set_y[i] == 'sick':\n",
        "      if sick < 14:\n",
        "        X.append(wordArry)\n",
        "        y.append(1)\n",
        "        sick = sick + 1\n",
        "      else:\n",
        "        t_X.append(wordArry)\n",
        "        t_y.append(0)\n",
        "    if training_set_y[i] == 'healthy':\n",
        "      if healthy < 14:\n",
        "        X.append(wordArry)\n",
        "        y.append(2)\n",
        "        healthy = healthy + 1\n",
        "      else:\n",
        "        t_X.append(wordArry)\n",
        "        t_y.append(0)\n",
        "    wordArry = []\n",
        "    k = k + 1\n",
        "    c = 0\n",
        "print(dry,\n",
        "sick,\n",
        "healthy)\n",
        "X_train = np.asarray(X, dtype=np.float32)\n",
        "y_train = np.asarray(y, dtype=np.float32)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "X_test = np.asarray(t_X, dtype=np.float32)\n",
        "y_test = np.asarray(t_y, dtype=np.float32)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlpF8kNnKaKL"
      },
      "source": [
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_test, y_test = np.array(X_test), np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0SLWbxWKeFu"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLJlHts6KgyO"
      },
      "source": [
        "# Part 2 - Building the RNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Initialising the RNN\n",
        "regressor = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaY_MtNyKivf"
      },
      "source": [
        "# Adding the first LSTM layer and some Dropout regularisation\n",
        "regressor.add(LSTM(units = 50,  activation='relu', return_sequences = True, input_shape = (50, 6272)))\n",
        "regressor.add(Dropout(0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IICxQlSeKl3t"
      },
      "source": [
        "# # Adding a second LSTM layer and some Dropout regularisation\n",
        "# regressor.add(LSTM(units = 50, return_sequences = True, activation='relu'))\n",
        "# regressor.add(Dropout(0.2))\n",
        "\n",
        "# # Adding a third LSTM layer and some Dropout regularisation\n",
        "# regressor.add(LSTM(units = 50, return_sequences = True, activation='relu'))\n",
        "# regressor.add(Dropout(0.2))\n",
        "\n",
        "# # Adding a fourth LSTM layer and some Dropout regularisation\n",
        "# regressor.add(LSTM(units = 50))\n",
        "# regressor.add(Dropout(0.2))\n",
        "\n",
        "# Adding the output layer\n",
        "# regressor.add(Dense(units = 1))\n",
        "regressor.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressor.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "# regressor.summary()\n",
        "\n",
        "# Fitting the RNN to the Training set\n",
        "cnn.fit(X_train, y_train, epochs = 150, batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGcKhueKKoup"
      },
      "source": [
        "\n",
        "xxxx = np.array(X_test)\n",
        "predict = regressor.predict(xxxx)\n",
        "a2 = 0\n",
        "a3 = 0\n",
        "a4 = 0\n",
        "for i in range(12):\n",
        "  if(0 == np.argmax(predict[i])):\n",
        "    a2 = a2 + 1\n",
        "  if(1 == np.argmax(predict[i])):\n",
        "    a3 = a3 + 1\n",
        "  if(2 == np.argmax(predict[i])):\n",
        "    a4 = a4 + 1\n",
        "print(a2+a3+a4)\n",
        "print(a2,a3,a4)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}